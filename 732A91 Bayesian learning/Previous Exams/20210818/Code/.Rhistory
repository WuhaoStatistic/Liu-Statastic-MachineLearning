b1 ~ dnorm( 0 , 5 ) , # samma normalfördelade prior till alla lutningspar.
logsigma ~ dnorm ( 0 , 5 )
)
flist_iv <- alist(
y ~ dnorm(mu, exp(logsigma)) , # likelihood från normalfördelning, km per liter
mu <- b0 + b1*x2, # hur mu är länkad till förklaringsvariabler.
b0 ~ dnorm( 10 , 5 ) , # normalfördelad prior för interceptet
b1 ~ dnorm( 0 , 5 ) , # samma normalfördelade prior till alla lutningspar.
logsigma ~ dnorm ( 0 , 5 )
)
Res_iii <- map(flist_iii, data=Forsaljning)
Res_iv <- map(flist_iv, data=Forsaljning)
compare(Res_1a,Res_1b,Res_iii,Res_iv,n=1e4,WAIC=FALSE)
# Uppgift 1e
Param_1e <- extract.samples(Res_1b)
xSeq <- sort(Forsaljning$x1)
mu.ci <- sapply( xSeq , function(x) PI( Param_1e$b0 + Param_1e$b1*x , prob=0.909) ) # mean(x2)=0
plot( Forsaljning$y ~ Forsaljning$x1 ,  type="n" )
shade( mu.ci , xSeq )
dens( rnorm(1e4, Param_1a$mu, exp(Param_1a$logsigma)) )
# Uppgift 1b
flist_1b <- alist(
y ~ dnorm(mu, exp(logsigma)) , # likelihood från normalfördelning, km per liter
mu <- b0 + b1*x1 + b2*x2, # hur mu är länkad till förklaringsvariabler.
b0 ~ dnorm( 10 , 5 ) , # normalfördelad prior för interceptet
c(b1,b2) ~ dnorm( 0 , 5 ) , # samma normalfördelade prior till alla lutningspar.
logsigma ~ dnorm ( 0 , 5 )
)
Res_1b <- map(flist_1b, data=Forsaljning)
precis(Res_1b) # sammanfattning av posteriorfördelningar
# Uppgift 1e
Param_1e <- extract.samples(Res_1b)
xSeq <- sort(Forsaljning$x1)
mu.ci <- sapply( xSeq , function(x) PI( Param_1e$b0 + Param_1e$b1*x , prob=0.909) ) # mean(x2)=0
plot( Forsaljning$y ~ Forsaljning$x1 ,  type="n" )
shade( mu.ci , xSeq )
### UPPGIFT 2 ###
Politiker <- readRDS("C:/Users/berwe48/Dropbox/Teaching/Bayesiansk statistik, 732g43/Hemtentamen 200403/Politiker.rds")
Modx1x2 <- alist( y ~ dbinom( 1 , p ) ,
logit(p) <- b0 + b1*x1 + b2*x2,
b0 ~ dnorm(0,5) ,
b1 ~ dnorm(0,5) ,
b2 ~ dnorm(0,5)
)
Modell_x1x2 <- map2stan(Modx1x2,data=Politiker)
# Uppgift 2a, räcker med 1000 MCMC-dragningar
precis(Modell_x1x2,prob=0.909)
plot(Modell_x1x2)
# Uppgift 2b
Param_2b <- extract.samples(Modell_x1x2)
xSeq <- sort(Politiker$x1)
p.ci <- sapply( xSeq , function(x) PI( logistic( Param_2b$b0 + Param_2b$b1*x + Param_2b$b2*1 ) , prob=0.952) ) # x2=0
plot( Politiker$y ~ Politiker$x1 ,  type="n" )
shade( p.ci , xSeq )
dens(post_2c / (1 - post_2c), main = "Oddset p/(p-1) fÃ¶r x1 = 0.5 och x2 = 0")
##### Uppgift 2c #####
# x2 = 0, dvs inte satt i riksdagen
post_2c <- logistic(sample_modell_2a$b0 + sample_modell_2a$b1 * 0.5 + sample_modell_2a$b2 * 0)
dens(post_2c / (1 - post_2c), main = "Oddset p/(p-1) fÃ¶r x1 = 0.5 och x2 = 0")
##### Uppgift 2b #####
# Observerade vÃ¤rden pÃ¥ x1 i ordning minst till stÃ¶rst
xSeq <- sort(data2$x1)
# Satt i riksdagen dvs x2 = 1
sample_modell_2a <- extract.samples(object = Modell_2a, n = 1e3)
# Skapar ett 95.2 procent kedibilitetsintervall fÃ¶r p som en funktion av x1 och x2 = 1
mu.ci <- sapply(X = xSeq,
FUN = function(X) PI(logistic(sample_modell_2a$b0 +
sample_modell_2a$b1 * X +
sample_modell_2a$b2 * 1),
prob=0.952))
# Skapar grunden fÃ¶r plotten. type = "n" dÃ¶ljer rÃ¥data
plot(data2$y ~ data2$x1, main = "95.2% kredibilitetsband", type = "n")
# Visualiserar kredibilitetsbandet
shade(object = mu.ci, lim =  xSeq, col = col.alpha("grey", 0.5))
##### Uppgift 2c #####
# x2 = 0, dvs inte satt i riksdagen
post_2c <- logistic(sample_modell_2a$b0 + sample_modell_2a$b1 * 0.5 + sample_modell_2a$b2 * 0)
dens(post_2c / (1 - post_2c), main = "Oddset p/(p-1) fÃ¶r x1 = 0.5 och x2 = 0")
##### Uppgift 2a #####
data2 <- readRDS("/Users/SidneyRydstrom/OneDrive - LinkoÌpings universitet/Bayesiansk/Omtenta/Politiker.rds")
flist_2a <- alist(y ~ dbinom( 1 , p ) ,
logit(p) <- b0 + b1*x1 + b2*x2,
c(b0,b1, b2) ~ dnorm(0,5)
)
Modell_2a <- map2stan(flist = flist_2a, data = data2,
iter = 6000, warmup = 1000, chains = 2, cores = 2)
# Kolla n_eff (Ã¶ver 100) och Rhat (1)
precis(Modell_2a, prob = 0.909)
# Tracerplot
tracerplot(Modell_2a)
##### Uppgift 2b #####
# Observerade vÃ¤rden pÃ¥ x1 i ordning minst till stÃ¶rst
xSeq <- sort(data2$x1)
# Satt i riksdagen dvs x2 = 1
sample_modell_2a <- extract.samples(object = Modell_2a, n = 1e3)
# Skapar ett 95.2 procent kedibilitetsintervall fÃ¶r p som en funktion av x1 och x2 = 1
mu.ci <- sapply(X = xSeq,
FUN = function(X) PI(logistic(sample_modell_2a$b0 +
sample_modell_2a$b1 * X +
sample_modell_2a$b2 * 1),
prob=0.952))
# Skapar grunden fÃ¶r plotten. type = "n" dÃ¶ljer rÃ¥data
plot(data2$y ~ data2$x1, main = "95.2% kredibilitetsband", type = "n")
# Visualiserar kredibilitetsbandet
shade(object = mu.ci, lim =  xSeq, col = col.alpha("grey", 0.5))
##### Uppgift 2c #####
# x2 = 0, dvs inte satt i riksdagen
post_2c <- logistic(sample_modell_2a$b0 + sample_modell_2a$b1 * 0.5 + sample_modell_2a$b2 * 0)
dens(post_2c / (1 - post_2c), main = "Oddset p/(p-1) fÃ¶r x1 = 0.5 och x2 = 0")
# Uppgift 2c
p_2c <- logistic( Param_2b$b0 + Param_2b$b1*0.5 )
Oddset <- p_2c/(1-p_2c)
dens(Oddset)
### UPPGIFT 3 ###
Restaurang <- readRDS("C:/Users/berwe48/Dropbox/Teaching/Bayesiansk statistik, 732g43/Hemtentamen 200403/Restaurang.rds")
set.seed(200403)
# Bayesiansk multilevel varierande-intercept Poisson regression
Pois_list <- alist( y ~ dpois(lambda) ,
log(lambda) <- a[Land] + b*x,
a[Land] ~ dnorm(mu,sigma) ,
mu <- dnorm(1,1) ,
sigma <- dcauchy(0,1) ,
b ~ dnorm(0,1)
)
Poisson_res <- map2stan(Pois_list,data=Restaurang,iter=40000) # 20000 MCMC-dragningar sparas
precis(Poisson_res,depth=2,prob=0.909)
plot(Poisson_res)
# Uppgift 3a
Param_3a <- extract.samples(Poisson_res)
dens(Param_3a$mu)
dens(Param_3a$mu)
dens(Param_3a$mu)
install.packages("LaplacesDemon")
install.packages("reshape2")
install.packages("pracma")
library(LaplacesDemon)
library(purrr)
library(ggplot2)
library("reshape2")
library(pracma)
##### Assignment 1: Linear and polynomial regression ######
data <- read.delim2("TempLinkoping.txt", header = TRUE, sep = "")
data$time <- as.numeric(paste(data$time))
data$temp <- as.numeric(paste(data$temp))
#a)
# Joint prior for beta and sigma^2
# Î²|sigma^2 â¼ N(Î¼0,sigma*inv(omega0))
# sigma^2 â¼ Invâchi2(Î½0,sigma0^2)
#Simulating prior for beta and sigma^2
my0 =  t(c(-10, 105, -105))
sigma20 = 1
i3 = diag(3)
omega0 = 0.1 * i3 #This parameter is sensitive to the spread of the temperature
v0 = 7
sigma2_prior <- 1/rchisq(n = 1000, v0,sigma20)
#simulate all the beta prior for each simulated sigma.
beta0_prior <- c()
beta1_prior <- c()
beta2_prior <- c()
for (i in 1:1000){
beta0_prior[i] <- c(rnorm(n = 1, mean= my0[1], sigma2_prior[i]*(inv(omega0))))
}
for (i in 1:1000){
beta1_prior[i] <- c(rnorm(n = 1, mean= my0[2], sigma2_prior[i]*(inv(omega0))))
}
for (i in 1:1000){
beta2_prior[i] <- c(rnorm(n = 1, mean= my0[3], sigma2_prior[i]*(inv(omega0))))
}
beta_prior <- data.frame(beta0 = beta0_prior, beta1 = beta1_prior, beta2 = beta2_prior)
par(mfrow=c(1,3))
plot(density(beta_prior$beta0), main = "Beta 0 prior density")
plot(density(beta_prior$beta1), main = "Beta 1 prior density")
plot(density(beta_prior$beta2), main = "Beta 2 prior density")
# Polynomial model
model <- function(beta,time){
#temp = Î²0 + Î²1 Â· time + Î²2 Â· time2 + Îµ, Îµiidâ¼ N(0, Ï2).
return(beta[[1]] + beta[[2]]*time + beta[[3]]*time^2)
}
#Fit the model the all simulated beta priors
fit.model <- matrix(nrow = length(data$time), ncol = 100)
for (i in 1:100) {
rprior <- sample(1:1000,1)
fit.model[,i] <- model(beta_prior[rprior,], data$time)
}
#Plot all models with the priors
fit.model_m <- melt(fit.model)
p <- ggplot() +
geom_point(aes(data$time*365,data$temp)) +
geom_line(data = fit.model_m, aes(x = Var1, y = value, group = Var2), alpha = 0.2) +
ylim(-20,30) +
ylab("Temperature") +
xlab("Day")
p
library(mvtnorm)
set.seed(123)
data <- read.table("TempLinkoping.txt",header=TRUE)
data2 <- cbind(data,time2=data$timeˆ2)
sigma0 <- 5 # .05 #
nu0 <- 10
mu0 <- c(-5,100,-100)
omega0 <- diag(c(5,2.5,5))
# omega0 <- diag(c(.01,.01,.01))
# plot(mu0[1] + data2$time*mu0[2] + data2$time2*mu0[3],col=2,type="l",
# xlab="day",ylab="Temperature",main="Mean prior regression curve")
N <- 100
rc <- rchisq(N,nu0)
sigmasim <- nu0*sigma0ˆ2/rc
betasim <- matrix(0,N,3)
for(i in 1:N) {
betasim[i,] <- rmvnorm(1,mu0,sigmasim[i]*solve(omega0))
}
plot(betasim[1,1] + data2$time*betasim[1,2] + data2$time2*betasim[1,3],type="l",
ylim=c(-10,40),xlab="day",ylab="Temperature",main="100 simulations of the regression \n curve from for(i in 2:N) {
lines(betasim[i,1] + data2$time*betasim[i,2] + data2$time2*betasim[i,3])
}
}
}
}
}
)
)
....
}}}}
}}}}}}}}}}}}}}}}}}}}}}}}}
)))))))))))))))))))
?array
mu_0 <- as.vector(rep(0,8))
Omega_0 <- (1/9)*diag(8)
v_0 <- 1
sigma2_0 <- 9
nIter <- 10000
library(mvtnorm)
X <- as.matrix(X)
PostDraws <- BayesLinReg(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter)
rm(list=ls())
source("ExamData.R")
set.seed(1)
setwd("C:/Users/berwe48/Dropbox/UNDERVISNING/KURSER/BayesLearn_LiU/OldExamsLiU/20210818/Code")
rm(list=ls())
source("ExamData.R")
set.seed(1)
knitr::opts_chunk$set(echo = TRUE,results="markup")
mu_0 <- as.vector(rep(0,8))
Omega_0 <- (1/9)*diag(8)
v_0 <- 1
sigma2_0 <- 9
nIter <- 10000
library(mvtnorm)
PostDraws <- BayesLinReg(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter)
X <- as.matrix(X)
PostDraws <- BayesLinReg(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter)
dim(PostDraws)
dim(PostDraws$betaSample)
Betas <- PostDraws$betaSample;quantile(Betas,probs=c(0.005,0.995))
?quantile
dim(Betas)
Mu_draws <- Betas*as.vector(c(1,1,1,0.5,0,1,0,1))
dim(Mu_draws)
Mu_draws <- Betas%*%as.vector(c(1,1,1,0.5,0,1,0,1))
dim(Mu_draws)
Diff_Exp_Price <- Betas[,7]-Betas[,8]
dens(Diff_Exp_Price)
plot(density(Diff_Exp_Price))
quantile(Diff_Exp_Price,probs=c(0.025,0.975))
Diff_Exp_Price <- Betas[,7]-Betas[,8]
quantile(Diff_Exp_Price,probs=c(0.025,0.975))
Diff_Exp_Price <- Betas[,7]-Betas[,8]
quantile(Diff_Exp_Price,probs=c(0.005,0.995))
?rnorm
Diff_Exp_Price <- Betas[,7]-Betas[,8]
quantile(Diff_Exp_Price,probs=c(0.005,0.995))
x1_grid <- seq(min(X[,2]),max(X[,2]),0.01)
ypred_South <- matrix(0,length(x1_grid),2)
ypred_Neither <- matrix(0,length(x1_grid),2)
for (ii in 1:length(x1_grid)){
Mu_South <- Betas%*%as.vector(c(1,x1_grid[ii],0,0,0,1,0,x1_grid[ii]))
Mu_Neither <- Betas%*%as.vector(c(1,x1_grid[ii],0,0,0,1,0,0))
ypred_South[ii,] <- quantile(rnorm(nIter,mean=Mu_South,sd=Sigma_draws),probs=c(0.025,0.975))
ypred_Neither[ii,] <- quantile(rnorm(nIter,mean=Mu_Neither,sd=Sigma_draws),probs=c(0.025,0.975))
}
Mu_draws <- Betas%*%as.vector(c(1,1,1,0.5,0,1,0,1))
Sigma_draws <- sqrt(PostDraws$sigma2Sample)
median(Mu_draws/Sigma_draws)
Diff_Exp_Price <- Betas[,7]-Betas[,8]
quantile(Diff_Exp_Price,probs=c(0.005,0.995))
x1_grid <- seq(min(X[,2]),max(X[,2]),0.01)
ypred_South <- matrix(0,length(x1_grid),2)
ypred_Neither <- matrix(0,length(x1_grid),2)
for (ii in 1:length(x1_grid)){
Mu_South <- Betas%*%as.vector(c(1,x1_grid[ii],0,0,0,1,0,x1_grid[ii]))
Mu_Neither <- Betas%*%as.vector(c(1,x1_grid[ii],0,0,0,1,0,0))
ypred_South[ii,] <- quantile(rnorm(nIter,mean=Mu_South,sd=Sigma_draws),probs=c(0.025,0.975))
ypred_Neither[ii,] <- quantile(rnorm(nIter,mean=Mu_Neither,sd=Sigma_draws),probs=c(0.025,0.975))
}
ypred_South
ypred_Neither
x1_grid <- seq(min(X[,2]),max(X[,2]),0.01)
ypred_South <- matrix(0,length(x1_grid),2)
ypred_Neither <- matrix(0,length(x1_grid),2)
for (ii in 1:length(x1_grid)){
Mu_South <- Betas%*%as.vector(c(1,x1_grid[ii],0,0,0,1,0,x1_grid[ii]))
Mu_Neither <- Betas%*%as.vector(c(1,x1_grid[ii],0,0,0,1,0,0))
ypred_South[ii,] <- quantile(rnorm(nIter,mean=Mu_South,sd=Sigma_draws),probs=c(0.025,0.975))
ypred_Neither[ii,] <- quantile(rnorm(nIter,mean=Mu_Neither,sd=Sigma_draws),probs=c(0.025,0.975))
}
ypred_South
x1_grid
Diff_Exp_Price <- Betas[,7]-Betas[,8]
quantile(Diff_Exp_Price,probs=c(0.005,0.995))
x1_grid <- seq(min(X[,2]),max(X[,2]),0.01)
ypred_South <- matrix(0,length(x1_grid),2)
ypred_Neither <- matrix(0,length(x1_grid),2)
for (ii in 1:length(x1_grid)){
Mu_South <- Betas%*%as.vector(c(1,x1_grid[ii],0,0,0,1,0,x1_grid[ii]))
Mu_Neither <- Betas%*%as.vector(c(1,x1_grid[ii],0,0,0,1,0,0))
ypred_South[ii,] <- quantile(rnorm(nIter,mean=Mu_South,sd=Sigma_draws),probs=c(0.025,0.975))
ypred_Neither[ii,] <- quantile(rnorm(nIter,mean=Mu_Neither,sd=Sigma_draws),probs=c(0.025,0.975))
}
plot(x1_grid,ypred_South[,1],"n",main="95 % posterior predictive intervals as a function of x1",
xlab="x1", ylab="",ylim=c(-2,4))
lines(x1_grid,ypred_South[,1],col="blue")
lines(x1_grid,ypred_South[,2],col="blue")
lines(x1_grid,ypred_Neither[,1],col="red")
lines(x1_grid,ypred_Neither[,2],col="red")
plot(density(Betas[,8]))
Diff_Exp_Price <- Betas[,7]-Betas[,8]
quantile(Diff_Exp_Price,probs=c(0.025,0.975))
plot(density(Betas[,8]))
Diff_Exp_Price <- Betas[,7]-Betas[,8]
plot(density(Diff_Exp_Price),type="l",main="Posterior distribution for the difference in expected price",xlab="beta_6-beta_7",ylab="")
quantile(Diff_Exp_Price,probs=c(0.025,0.975))
plot(density(Betas[,8]),type="l",main="Posterior distribution",xlab="beta_7",ylab="")
Mu_draws <- Betas%*%as.vector(c(1,-0.5,-0.5,0,0,1,0,-0.5))
plot(density(Mu_draws),type="l",main="Posterior distribution of mu",xlab="mu",ylab="")
mean(Mu_draws>0)
y1 <- c(2.32,1.82,2.40,2.08,2.13)
n <- length(y1)
theta <- rgamma(1e4,shape = 2*n+1,rate = 0.5+sum(y1))
y_tilde <- rgamma(1e4, shape = 2, rate = theta)
plot(density(y_tilde),type="l",main="Posterior distribution",xlab="y_tilde",ylab="")
mean(y_tilde < 1.9)
sum(y1)
mean(y_tilde > 2.4)
y_tilde
y1 <- c(2.32,1.82,2.40,2.08,2.13)
n <- length(y1)
theta <- rgamma(1e4,shape = 2*n+1,rate = 0.5+sum(y1))
y_tilde <- rgamma(1e4, shape = 2, rate = theta)
plot(density(y_tilde),type="l",main="Posterior distribution",xlab="y_tilde",ylab="")
mean(y_tilde < 1.9)
mean(y_tilde > 2.4)
30*0.3431
y1 <- c(2.32,1.82,2.40,2.08,2.13)
n <- length(y1)
theta <- rgamma(1e5,shape = 2*n+1,rate = 0.5+sum(y1))
y_tilde <- rgamma(1e5, shape = 2, rate = theta)
plot(density(y_tilde),type="l",main="Posterior distribution",xlab="y_tilde",ylab="")
mean(y_tilde < 1.9)
mean(y_tilde > 2.4)
mean(y_tilde > 2.4)*30
nSim <- 10000
nWeeks <- 30
WeeklyWeights <- matrix(NA,nSim,nWeeks)
for (i in 1:nSim){
thetas <- rgamma(nWeeks,shape = 2*n+1,rate = 0.5+sum(y1))
WeeklyWeights[i,] <- t(rgamma(nWeeks, shape = 2, rate = thetas))
}
ExceedingWeeks <- rowSums(WeeklyWeights > 2.4)
mean(ExceedingWeeks)
nSim <- 10000
nWeeks <- 30
WeeklyWeights <- matrix(NA,nSim,nWeeks)
for (i in 1:nSim){
thetas <- rgamma(nWeeks,shape = 2*n+1,rate = 0.5+sum(y1))
WeeklyWeights[i,] <- t(rgamma(nWeeks, shape = 2, rate = thetas))
}
ExceedingWeeks <- rowSums(WeeklyWeights > 2.4)
mean(ExceedingWeeks)
nSim <- 1e5
nWeeks <- 30
WeeklyWeights <- matrix(NA,nSim,nWeeks)
for (i in 1:nSim){
thetas <- rgamma(nWeeks,shape = 2*n+1,rate = 0.5+sum(y1))
WeeklyWeights[i,] <- t(rgamma(nWeeks, shape = 2, rate = thetas))
}
ExceedingWeeks <- rowSums(WeeklyWeights > 2.4)
mean(ExceedingWeeks)
mu_0 <- as.vector(rep(0,8))
Omega_0 <- (1/9)*diag(8)
v_0 <- 1
sigma2_0 <- 9
nIter <- 10000
library(mvtnorm)
X <- as.matrix(X)
PostDraws <- BayesLinReg(y, X, mu_0, Omega_0, v_0, sigma2_0, nIter)
Betas <- PostDraws$betaSample
quantile(Betas[,2],probs=c(0.005,0.995))
Diff_Exp_Price <- (Betas[,5]+Betas[,7])-(Betas[,6]+Betas[,8])
plot(density(Diff_Exp_Price),type="l",
main="Posterior distribution for the difference in expected price",
xlab="",ylab="")
quantile(Diff_Exp_Price,probs=c(0.025,0.975))
plot(density(Betas[,8]),type="l",main="Posterior distribution",xlab="beta_7",ylab="")
Mu_draws <- Betas%*%as.vector(c(1,1,1,0.5,0,1,0,1))
Sigma_draws <- sqrt(PostDraws$sigma2Sample)
median(Sigma_draws/Mu_draws)
# b)
ndraws = 10000
thetas = rep(0, ndraws)
for(i in 1:ndraws) {
theta = rgamma(1, shape = 11, rate = 11.25)
thetas[i] = rgamma(1, shape = 2, rate = theta)
}
# plot simulated draws
plot(density(thetas))
# Pr(Y < 1.9 | y1 ... y5) = 0.53
prob = sum(thetas < 1.9) / ndraws
# c)
draws_c = rep(0, ndraws)
for(i in 1:ndraws) {
count = 0
for(j in 1:30) {
theta = rgamma(1, shape = 11, rate = 11.25)
count = count + (rgamma(1, shape = 2, rate = theta) > 2.4)
}
draws_c[i] = count
}
# expected weeks is around 10
expected_weeks = mean(draws_c)
expected_weeks
# d)
# posterior expected loss using simulated draws
a = seq(1, 20, 0.02)
exp_loss = rep(0, length(a))
for(i in 1:length(a)) {
weight = 0.9 * log(a[i])
break_ratio = mean(thetas > weight)
exp_weeks = break_ratio * 30
exp_loss[i] = a[i] + exp_weeks
}
# plot loss as a function of cost (a)
plot(a, exp_loss)
# optimal cost, minimize loss w.r.t. a, a = 7
optimal_a = a[which(exp_loss == min(exp_loss))]
optimal_a
ndraws = 10000
mu_0 = rep(0, 8)
omega_0 = solve(9 * diag(8))
v0 = 1
sigma2_0 = 9
draws = BayesLinReg(y, as.matrix(X), mu_0, omega_0, v0, sigma2_0, ndraws)
betas = draws$betaSample
# a)
# the 99% equal tail credible interval is [-0.32, 1.85], and it is normally
# distributed which means that most of the values are positive. This means
that
# larger number of square meters generally means that the apartment is more
# expensive.
cred_int = quantile(betas[,2], c(0.005, 0.995))
cred_int
# b)
y = betas[,1] + betas[,2] * 1 + betas[,3] * 1 + betas[,4] * 0.5 + betas[,6] *
1 + betas[,8] * 1
CV = var(y) / mean(y) # 0.14
CV
# c)
exp_price_inner = mean(betas[,1] + betas[,2] + betas[,5] + betas[,7])
exp_price_south = mean(betas[,1] + betas[,2] + betas[,6] + betas[,8])
# the expected selling price for apartments in the inner city is alot higher
# than that of apartments south of the city.
x1 = seq(0, 10, 0.1)
exp_south = rep(0, length(x1))
exp_neither = rep(0, length(x1))
for(i in 1:length(x1)) {
exp_south[i] = mean(betas[,1] + betas[,2] * x1[i] + betas[,6] + betas[,8] *
x1[i])
exp_neither[i] = mean(betas[,1] + betas[,2] * x1[i])
}
plot(x1, exp_south, type = "l", col = "blue")
lines(x1, exp_neither, col = "red")
# d)
mus_south = betas[,1] - 0.5 * betas[,2] - 0.5 * betas[,3] + betas[,6] - 0.5 *
betas[,8]
post_mean = mean(mus_south)
plot(density(mus_south))
# e)
x1 = seq(min(X[,2]), max(X[,2]), 0.01)
sigmas = sqrt(draws$sigma2Sample)
y_cred_ints = matrix(0, length(x1), 2)
for(i in 1:length(x1)) {
mu = betas[,1] + betas[,2] * x1[i] + betas[,3] + 0.5 * betas[,4] +
betas[,5] + betas[,7] * x1[i]
y = rnorm(10000, mu, sigmas)
y_cred_ints[i,] = quantile(y, c(0.025, 0.975))
}
# plot 95% cred interval for y draws
plot(x1, y_cred_ints[,1], col = "blue", ylim = c(-3, 6))
points(x1, y_cred_ints[,2], col = "red")
# d)
log_posterior <- function(theta, n, sum_lnx) {
return(-(theta - sum_lnx / n)^2 * n) # log prior is 0
}
theta_grid = seq(-1, 2, 0.01)
posterior = exp(log_posterior(theta_grid, 5, 2)) # data from (b)
posterior_dist = posterior / (sum(0.01 * posterior)) # normalize
# plot posterior distribution
plot(theta_grid, posterior_dist)
numeric(8)
1.25/2
0.625*3
solve(9*diag(1,8))
97/13
