{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **0. Some hardware setup**\n",
    "Keras uses all available GPUs in your computer. The following ```os.environ``` commands configures that only one of them should be used. If you are on a system with several GPUs and want to use more than one, you can change or comment out these commands.\n",
    "\n",
    "By default, Keras will allocate all of the available memory in the device. The last two lines will have Keras allocate memory as needed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import tensorflow as tf\n",
    "\n",
    "# If there are multiple GPUs and we only want to use one/some, set the number in the visible device list.\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\n",
    "\n",
    "# This sets the GPU to allocate memory only as needed\n",
    "physical_devices = tf.config.experimental.list_physical_devices('GPU')\n",
    "if len(physical_devices) != 0:\n",
    "    tf.config.experimental.set_memory_growth(physical_devices[0], True) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **1. Introduction to Keras**\n",
    "Keras is a Python API (Application Programming Interface) for fast development of neural networks that sits on top of TensorFlow, the machine learning platform developed by Google. The full documentation, which you probably will have to search at some point, can be found at https://www.tensorflow.org/api_docs/python/tf/keras.\n",
    "\n",
    "To begin we should go through some of the essential terminology that you need to know for the rest of the assignment to go smoothly.\n",
    "\n",
    "1. **Models**\n",
    "    \n",
    "    A Keras Model is, just like the name implies, the top-level object that describes your neural network architecture. This is the thing you create, train, and use to process data. It is very general and can be configured to perform essentially any task you want, such as image classification, text analysis, or continuous regression.\n",
    "    \n",
    "\n",
    "2. **Layers**\n",
    "    \n",
    "    These are the fundamental building blocks of the Model. Each Model contains a list of Layers. The way these are connected to each other defines the architecture of the network. There is a huge number of Layers, such as ```Dense```, which is the same fully connected layer you implemented in assignment 1. Another very important Layer is ```Conv2D```, which performs 2-dimensional convolution of the input using some filter.\n",
    "    \n",
    "    \n",
    "3. **Optimizers, Losses, and Metrics**\n",
    "\n",
    "    These are the functions and algorithms used to train and evaluate the Model.\n",
    "    * The Optimizer defines the algorithm used to update the weights using the gradients. In the first assignment you implemented stocastic gradient descent (SGD), which is one type of Optimizer. \n",
    "    * Losses are differentiable objective functions that compute the performance quantity that the model tries to minimize. One example of a loss function is Mean Squared Error, which you used in the first assignment. Another is Categorical Crossentropy (*aka.* log-loss) which we will use this time.\n",
    "    * Metrics are functions that compute the performance of the network in a way that is humanly understandable. Unlike the Losses, Metrics don't need to be differentiable since we don't use them in the gradient calculations. A perfect example of this is Accuracy; it's easily understood, but we can't use it as a Loss function.\n",
    "    \n",
    "We will look at all of this in more detail further down."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **2. Loading the dataset**\n",
    "For this introduction, we will use the MNIST dataset as for assignment 1. This time however, we will use the higher resolution images. We start by importing Keras and loading the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "\n",
    "(X_train, y_train), (X_test, y_test) = keras.datasets.mnist.load_data()\n",
    "\n",
    "print(\"Shape of training data:\")\n",
    "print(X_train.shape)\n",
    "print(y_train.shape)\n",
    "print(\"Shape of test data:\")\n",
    "print(X_test.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Plotting images**\n",
    "In order to create nice plots in Python we use a library called *matplotlib*. As the name suggests, this gives access to Matlab-like plot functions, although without the interactive elements. When we call ```plt.show()``` the current figure is rendered as a *.png* and displayed.\n",
    "\n",
    "Here we select some random examples from the X_train matrix and show them as images (you might need to run the cell twice to see the images)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, axarr = plt.subplots(1, 5, figsize=(16,3))\n",
    "\n",
    "for i in range(5):\n",
    "    rnd = np.random.randint(low=0, high=X_train.shape[0])\n",
    "    img = X_train[rnd]\n",
    "    axarr[i].imshow(img, cmap=plt.get_cmap('gray'))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Preparing the dataset**\n",
    "We need to make some transformations to the data before we can use it for training. It's possible to use the data as is, but the results will not be very good. We will leave it as an exercice as to why that is the case.\n",
    "\n",
    "The first step is to change the labels from a number representation, i.e. 1,2,3 etc., to a ***one-hot encoding*** where each target is a vector with only one of the values set to 1, the rest 0. This represents the probability that the output from the network should try to mimic. The concept is the same as the <strong>D</strong> matrix from the first assignment. A minor difference is that in **D**, each target was a vector with a single 1 and the rest -1. This is because we used *tanh* as output activation, which outputs in the [-1, 1] range. In this assignment we use *softmax* activation in the output layer, which only give values beween 0 and 1, thus the one-hot encoding.\n",
    "\n",
    "<small>*As a side note for those of you that are interested. While the change from a -1/1 vector to a 0/1 (one-hot) vector might not seem that significant at first, the gradient calculation is very nice when using both softmax activation and cross-entropy loss in the output layer, as several terms cancel. We will let Keras do all the work for us this time, but if you ever need to implement a general backpropagation algorithm from scratch, or you just really like partial derivatives :), this is something to look into.*</small>\n",
    "\n",
    "Second, we intensity normalize the data to the [0,1] range, instead of [0,255]. This is not strictly necessary as all weights will just scale to account for the change, but the convergence is much faster. The reason for this is that Keras uses specific initialization schemes for the weights that expect the data to be [0,1] normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transform label indices to one-hot encoded vectors\n",
    "y_train_c = keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test_c  = keras.utils.to_categorical(y_test, num_classes=10)\n",
    "\n",
    "# Normalization of pixel values (to [0-1] range)\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "\n",
    "# Print some labels\n",
    "print(\"Label index --> one-hot encoding\")\n",
    "for i in range(5):\n",
    "    print(str(y_train[i]) + \" --> \" + str(y_train_c[i]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **3. A first Keras model**\n",
    "Now let's build our first Model for classifying the MNIST data. In Keras, there are two different ways to create a model. The first is using the **Sequential** API which can only build networks that have a single stack of layers. We can add new layers to the network by calling ```model.add(...)```. This is nice and simple but is limited to a single stack of layers.\n",
    "\n",
    "*Why would you need anything else, you might ask. Nowadays most models for deep learning connect layers not just sequentially, but also using longer connection that \"skip\" one or more layers before merging into the network again. These, called skip connections or residual connections, are very powerful but also outside the scope of this course. For a concrete example lookup the popular ResNet architecture.*\n",
    "\n",
    "The second way to build models is using the **Functional** API. In this, we treat each layer as an individual function and we manually use the output from one (or more) layer as the input to another layer. This gives much more flexibility in designing the network architecture. For this reason, we will use the Functional API even though the Sequential would do just fine for this assignment. This will give you more freedom in the final task and better prepare you for any future projects you want to do.\n",
    "\n",
    "We will begin by building a model very similar to the one from assignment 1, *i.e.* a two-layer fully connected (Dense) network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import some stuff we need\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense, Flatten\n",
    "\n",
    "# Create the input layer\n",
    "x_in = Input(shape=X_train.shape[1:])\n",
    "\n",
    "# Create the rest of the layers\n",
    "x = Flatten()(x_in)\n",
    "x = Dense(64, activation='tanh')(x)\n",
    "x = Dense(10, activation='softmax')(x)\n",
    "\n",
    "# Create the model object\n",
    "model = Model(inputs=x_in, outputs=x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We want you to really understand what is going on here, so let's go through this step by step. \n",
    "1. ```x_in = Input(shape=X_train.shape[1:])```: this creates the input layer using the ```Input``` class. This requires the shape of the input data, which is given by ```X_train.shape[1:]```. We use ```[1:]``` to select only the height and width of the images, skipping the number of images. If this is unfamiliar to you, we recommend section 3.1.3 in the official python tutorial: https://docs.python.org/3/tutorial/introduction.html. The output from the input layer, which we call ```x_in```, is a data structure that can be used as input to other layers.\n",
    "\n",
    " *As a side note, normally when we think of the output of a function we expect that to be some data that we can plot or visualize in some way, like the value 5, or a vector, etc. However, remember that we have not given any data to this model yet, so there is no data to visualize. Here instead we are defining how to process the input data that will eventually be given to the model. When feeding the input of a layer to another, we add a new operation to our process. Remember, a neural network is just a function. It can be a very complicated function, but a function nonetheless, and is therefore just a series of operations.* \n",
    " \n",
    "\n",
    "2. ```x = Flatten()(x_in)```: simply changes the shape of the input data. The MNIST dataset is images of size 28x28 pixels. However, the next ```Dense``` layer expects all features in a single dimension. The flatten operation simply squashes multidimentional arrays into a single vector (in our case from 28x28 to 784). Note that when we add the ```Flatten``` layer we directly give it the input in the same line of code, *i.e.* ```x = Flatten()(x_in)```. This is a nice trick because it means we don't have to save the layer itself to a variable before using it; we can just define and use it on the same line which saves both space and time. You can test this by running the following cell, which creates a standalone ```Flatten``` layer and processes the first 5 images in X_train. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST for the Flatten() layer\n",
    "F = Flatten(input_shape=(28,28))\n",
    "\n",
    "''' don't worry about the following operation. This it to make sure that the data \n",
    "is in the right format to be processed by Keras. This is automatically taken care \n",
    "of when using the input layer.'''\n",
    "In = tf.convert_to_tensor(X_train[0:5])\n",
    "Out = F(In)\n",
    "\n",
    "print(\"Before Flatten: \" + str(In.shape))\n",
    "print(\"After Flatten : \" + str(Out.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. ```x = Dense(64, activation='tanh')(x)``` and ```x = Dense(10, activation='softmax')(x)```: defines two dense layers, the first with 64 nodes and the second with 10, as the number of output classes. For the hidden layer we specify *tanh* as activation function , whereas *softmax* for the output layer.\n",
    "\n",
    "    Specifying the activation directly when creating the layer is not the only way to do it; there is a layer called ```Activation``` that can be used to add a standalone activation function between layers. This is sometimes necessary, for example when using more advanced activation functions, such as LeakyReLU. We also sometimes want to add normalization layers within the network to improve the training, which we will see in the main assignment. In these cases we usually apply the activation function after normalization, which requires using a separate ```Activation``` layer.\n",
    "\n",
    "\n",
    "4. ```model = Model(inputs=x_in, outputs=x)```: creates the actual model object. We initialize it with the input and output of the network, which we have in ```x_in``` and ```x```. At the moment we don't need any outputs from the middle layers, which is why we overwrite ```x``` when adding new layers. However, we must always save a separate variable for the input layer to create the model.\n",
    "\n",
    "\n",
    "Notice how we only specify the shape of the data at the first layer, using ```X_train.shape[1:]```. Although every layer has this input parameter, we only need to specify the input shape of the first layer since the model takes care of the rest for us.  No more struggling with matrix sizes. Neat!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Finalizing the model**\n",
    "Now that we defined the architecture of our model, we need to specify the way we want to train and evaluate it, *i.e.* the Optimizer, Loss function and any Metrics. As Optimizer we will use Mini-batch Stocastic Gradient Descent, which is implemented in the ```SGD``` class. This is almost the same as in assignment 1, except that we don't process the entire dataset before taking a learning step. Instead we randomly divide the data in mini-batches of a few examples, typically a relatively small power of 2, and take a step for each of them. This makes learning faster since we take more steps per epoch than if we were to use the entire dataset for each step. It's also sometimes necessary since the dataset can be so large that it doesn't fit in memory. In other words, batch SGD takes one step based on the mean of the gradient from all data, whereas mini-batch SGD takes multiple steps where each is the mean computed over a fixed number of samples (batch size).\n",
    "\n",
    "We will also use Nesterov momentum and learing rate decay in the optimizer. Don't worry if this doesn't mean anything to you, it's just a specific improvement to the base SGD algorithm that tries to be smart about the learning step. You will see that it is very good for this problem.\n",
    "\n",
    "We will use *categorical crossentropy* as loss function, which uses the *log* of predicted probabilites to calculate the loss, which exponentially punishes predictions the more wrong they are. For the metric we will use accuracy, *i.e.* the fraction of the data classified correctly. Note that we can give several metrics to the model if we want, but in this case we will only use accuracy.\n",
    "\n",
    "We set the optimizer, loss function, and metrics using the ```model.compile``` method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.optimizers import SGD\n",
    "\n",
    "# Define the optimizer\n",
    "sgd = SGD(lr=0.01, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "\n",
    "# Compile the model using the optimizer, loss and metric\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Inspecting the model**\n",
    "Now that we have a model, we can do some visualization. First, let's print a table summarizing the model with ```model.summary``` (the parameter is simply the width of the table)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see the type, output size, and number of parameters for each layer. This is very useful when working with larger models. For example we might want to find out why the model is overfitting. Is there a layer with a lot of parameters? That might be a good candidate to change. Having many parameters to train is also slower, so changing the network by adding or removing layers might speed up training. Finally there's a summary at the bottom with the total number of parameters. Some layers have parameters that are configurable but not part of the optimization, these are the *Non-trainable params* of which we currently have none.\n",
    "\n",
    "Note how the first value of the output shape is **None**. This just means that we haven't specified the mini-batch size yet. We do this when we actually train the model.\n",
    "\n",
    "We can also print an image of the model using the code below. This might seem unnecessary when we have the table, and at the moment it gives roughly the same information. However, when building more advanced models, for example using residual connections, the table quickly becomes unreadable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model, show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Training the model**\n",
    "Finally, time to do some model training! This is done with the ```model.fit``` method. We input the training data, one-hot encoded targets, mini-batch size, and number of epochs. When using mini-batches, one epoch passes when all the data has been used once. Each training sample can only be used once per epoch, thus the mini-batches are randomly selected each epoch, which is why it's called stochastic gradient decent.\n",
    "\n",
    "We also set the *validataion_split* parameter to 0.2. This tells ```model.fit``` to split off 20% of the training data to use for validation during the training process. In the previous assignment we used the test data for this purpose, but this is not how it's usually done. We might want to use the validation data during training to make decisions, such as stopping early if we detect overfitting or decreasing the learning rate if we detect that the performance no longer improves. But that means we cannot use the same data to get the final performance of the model, as it has influenced the training and the model parameters. This is why we need three datasets, one for training, one for validation during training, and one for testing after training. Using the *validation_split* parameter we get a random selection from the training data for validation, but we can also give a specific validation set to the ```fit``` method if we want, or we can tell it to always use the first 20%, etc. \n",
    "\n",
    "Finally, setting the *verbose* flag to 1 prints the status of the training. Now let's run it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(X_train, y_train_c, epochs=10, batch_size=32, verbose=1, validation_split=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### **Evaluating the model**\n",
    "We evaluate the model on the test data using the ```model.evaluate``` method, which returns the loss and metrics. The results can vary, but should generally be around 97% accuracy. Remember that the first assignment required only 93%. Not bad for a few lines of code and 10 epochs, right?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = model.evaluate(X_test, y_test_c, verbose=0)\n",
    "\n",
    "for i in range(len(score)):\n",
    "    print(\"Test \" + model.metrics_names[i] + \" = %.3f\" % score[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the history of the training using the *history* object that is returned from ```model.fit```. This again uses the *matplotlib* library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12,5))\n",
    "\n",
    "# Plot loss\n",
    "plt.subplot(1,2,1)\n",
    "plt.semilogy(history.history['loss']    , label=\"Training\")\n",
    "plt.semilogy(history.history['val_loss'], label=\"Validation\")\n",
    "plt.title('Model loss')\n",
    "plt.ylabel('Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='upper right')\n",
    "plt.grid(True, which=\"both\")\n",
    "\n",
    "# Plot accuracy\n",
    "plt.subplot(1,2,2)\n",
    "plt.plot(100 * np.array(history.history['accuracy'])    , label=\"Training\")\n",
    "plt.plot(100 * np.array(history.history['val_accuracy']), label=\"Validation\")\n",
    "plt.title('Model accuracy')\n",
    "plt.ylabel('Acc [%]')\n",
    "plt.xlabel('Epoch')\n",
    "plt.legend(loc='lower right')\n",
    "plt.grid(True, which=\"both\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also import some libraries to calculate and view the confusion matrix. For this we want to get the predicted class for each test sample, which we can do with ```model.predict```. However, since the model outputs a 10-class probabilty vector we need to convert it back to a label index vector (*i.e.* 1,2,3 etc.). Use ```np.argmax``` for this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "from Custom import PrintPrediction\n",
    "\n",
    "# Probabilities for each class, for each test image\n",
    "prob_test = model.predict(X_test)\n",
    "# Prediction (class number) for each test image.\n",
    "p_test = np.argmax(prob_test,axis=1)\n",
    "# Calculate confusion matrix\n",
    "CM = confusion_matrix(y_test, p_test)\n",
    "\n",
    "# Print probablities and predictions (rounded to a few decimal places)\n",
    "print(\"Probabilites and predictions\")\n",
    "for i in range(5):\n",
    "    PrintPrediction(prob_test[i])\n",
    "    \n",
    "print(\"\\nConfusion matrix\")\n",
    "print(CM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a nice custom function for evaluating the model and plotting the history and confusion matrix. We will use this in the main part of the assignment, so you can focus on the fun parts instead of writing the plot code.\n",
    "\n",
    "*The code for the Labels input is a bit of python magic called list comprehention. It's very nice and also very easy to make completly unreadable. Use responsibly :)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Custom import PlotModelEval\n",
    "plt.text\n",
    "PlotModelEval(model, history, X_test, y_test, Labels=[str(x) for x in range(10)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **------------------------------------------------------------------------------------**\n",
    "### **Extra, examples of more complicated models**\n",
    "As a bonus, here are some short examples of models built using the functional API, that can't be created with the Sequential model.\n",
    "\n",
    "##### **Adding a residual (skip) connection**\n",
    "Residual (skip) connection are used in some of the most powerful modern architectures, for reasons that are outside the scope of this course. However, for a concreate example, lookup ResNet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Keras and create some shorthand notation\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as L\n",
    "\n",
    "# Begin like last time\n",
    "x_in = L.Input(shape=100)\n",
    "x = L.Dense(50, activation=\"tanh\")(x_in)\n",
    "\n",
    "# But now, save the output to another variable, y\n",
    "y = L.Dense(10, activation=\"tanh\")(x)\n",
    "\n",
    "# Use y as new input, but then change back to x\n",
    "x = L.Dense(10, activation=\"tanh\")(y)\n",
    "x = L.Dense(10, activation=\"tanh\")(x)\n",
    "\n",
    "# Now we have two different outputs from the network, at two different points.\n",
    "# We can merge them using different Layers, for example Add, using a list of layer outputs:\n",
    "x = L.Add()([x,y])\n",
    "\n",
    "# Finally, add the output layer and create the model\n",
    "x = L.Dense(5, activation=\"softmax\")(x)\n",
    "model2 = keras.models.Model(inputs=x_in, outputs=x)\n",
    "\n",
    "# Print the model\n",
    "model2.summary(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can see on the right that there is a new column the shows the connections between layers. That's not very visual though, so let's print it as an image instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keras.utils.plot_model(model2, show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **More than one input and output**\n",
    "This can be useful when doing object recognition in images. You can imagine that you not only want to classify an image as a cat or dog, but also give the coordinates of the cat or dog in the image. Or even have a segmented images as output where the background is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Keras and create some shorthand notation\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as L\n",
    "\n",
    "x_in1 = L.Input(shape=100)\n",
    "x_in2 = L.Input(shape=50)\n",
    "\n",
    "x1 = L.Dense(20, activation=\"tanh\")(x_in1)\n",
    "x2 = L.Dense(10, activation=\"tanh\")(x_in2)\n",
    "c1 = L.Concatenate()([x1,x2])\n",
    "x3 = L.Dense(10, activation=\"tanh\")(c1)\n",
    "x4 = L.Dense(10, activation=\"tanh\")(c1)\n",
    "\n",
    "model3 = keras.models.Model(inputs=[x_in1, x_in2], outputs=[x3,x4])\n",
    "\n",
    "# Print the model\n",
    "keras.utils.plot_model(model3, show_shapes=True, show_layer_names=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### **A (not so) crazy model**\n",
    "\n",
    "You can also very easily make some crazy models. Here is a network where the input to each layer is the sum of outputs from all previous layers. While this particular model is just a dummy, similar techniques where there is a high degree of connectivity between the layers have actually been used in some scientific publications, for example to detect diseases in X-Ray images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import Keras and create some shorthand notation\n",
    "import tensorflow.keras as keras\n",
    "import tensorflow.keras.layers as L\n",
    "\n",
    "# Begin like last time\n",
    "x_in = L.Input(shape=100)\n",
    "\n",
    "x1 = L.Dense(10, activation=\"tanh\")(x_in)\n",
    "x2 = L.Dense(10, activation=\"tanh\")(x1)\n",
    "a1 = L.Add()([x1,x2])\n",
    "x3 = L.Dense(10, activation=\"tanh\")(a1)\n",
    "a2 = L.Add()([x1,x2,x3])\n",
    "x4 = L.Dense(10, activation=\"tanh\")(a2)\n",
    "a2 = L.Add()([x1,x2,x3,x4])\n",
    "\n",
    "x = L.Dense(5, activation=\"tanh\")(a2)\n",
    "model4 = keras.models.Model(inputs=x_in, outputs=x)\n",
    "\n",
    "# Print the model\n",
    "keras.utils.plot_model(model4, show_shapes=True, show_layer_names=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
